{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf6f289b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from IPython.display import Markdown\n",
    "from IPython.display import display\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import glob\n",
    "import chromadb\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "from google.api_core import retry\n",
    "from langcodes import Language\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple\n",
    "import fasttext\n",
    "import re\n",
    "from typing import Any, Dict, List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ad699fd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_language_name(lang_code):\n",
    "    \"\"\"Returns the full English name of a language code.\"\"\"\n",
    "    language = Language.make(language=lang_code).language_name()\n",
    "    return language\n",
    "\n",
    "def import_google_api():\n",
    "    load_dotenv()\n",
    "    GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "    client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "    # Simple model check (keeping the original print logic)\n",
    "    for m in client.models.list():\n",
    "        if \"embedContent\" in m.supported_actions:\n",
    "            print(m.name)\n",
    "\n",
    "    return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b4171d9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def embedding_function(client):\n",
    "    class GeminiEmbeddingFunction(EmbeddingFunction):\n",
    "        document_mode = True\n",
    "\n",
    "        def __init__(self, client):\n",
    "            self.client = client\n",
    "            # Retry only on specific transient API errors\n",
    "            self._retry = retry.Retry(predicate=lambda e: isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
    "\n",
    "        def __call__(self, input: Documents) -> Embeddings:\n",
    "            embedding_task = \"retrieval_document\" if self.document_mode else \"retrieval_query\"\n",
    "            response = self._retry(self.client.models.embed_content)(\n",
    "                model=\"models/text-embedding-004\",\n",
    "                contents=input,\n",
    "                config=types.EmbedContentConfig(task_type=embedding_task),\n",
    "            )\n",
    "            return [e.values for e in response.embeddings]\n",
    "\n",
    "    return GeminiEmbeddingFunction(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fb3f4f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Assuming 'Document' is a class/dataclass with 'page_content' and 'metadata' attributes\n",
    "# from a library like LangChain, LlamaIndex, etc.\n",
    "class Document:\n",
    "    def __init__(self, page_content: str, metadata: dict = None):\n",
    "        self.page_content = page_content\n",
    "        self.metadata = metadata if metadata is not None else {}\n",
    "\n",
    "# NOTE: The original doc_stats, filename_base, google_drive_path, and all_chunks \n",
    "# are not defined in the provided snippet. I'll define placeholders or \n",
    "# make assumptions for a runnable example.\n",
    "\n",
    "def parse_markdown_for_metadata(directory: str, google_drive_path: str = None) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Reads markdown files in a directory (and subdirectories) and creates a \n",
    "    single Document for each file, adding relevant metadata, but does not chunk.\n",
    "    \"\"\"\n",
    "    markdown_files = glob.glob(os.path.join(directory, '**/*.md'), recursive=True)\n",
    "    if not markdown_files:\n",
    "        print(\"No markdown files found\")\n",
    "        return []\n",
    "\n",
    "    print(f\"Processing {len(markdown_files)} markdown files...\")\n",
    "    \n",
    "    all_documents = []\n",
    "\n",
    "    for filepath in tqdm(markdown_files, desc=\"Processing documents\"):\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                markdown_text = f.read()\n",
    "        except Exception as e:\n",
    "            print(f\"\\nWarning: Could not read file {filepath}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Extract basic file name for metadata\n",
    "        filename_base = os.path.basename(filepath)\n",
    "\n",
    "        # Create a single Document for the entire file content\n",
    "        doc = Document(page_content=markdown_text)\n",
    "\n",
    "        # Add metadata\n",
    "        doc.metadata[\"source\"] = filename_base\n",
    "        # Use google_drive_path if provided, otherwise use local path\n",
    "        doc.metadata[\"source_path\"] = google_drive_path or filepath \n",
    "        \n",
    "        # NOTE: Since we are not chunking by headers, we won't have a specific header.\n",
    "        # We set it to an empty string or a placeholder.\n",
    "        doc.metadata[\"header\"] = \"\" \n",
    "        \n",
    "        # Since the entire file is one document, these values reflect that.\n",
    "        doc.metadata[\"chunk_index\"] = 0\n",
    "        doc.metadata[\"total_chunks\"] = 1\n",
    "        doc.metadata[\"is_complete_doc\"] = True\n",
    "        \n",
    "        all_documents.append(doc)\n",
    "\n",
    "    print(f\"\\nSuccessfully processed {len(all_documents)} files into documents.\")\n",
    "    return all_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af21f7b0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_collection(chroma_client, gemini_embedding_function, documents_list):\n",
    "    \"\"\"\n",
    "    Create or update ChromaDB collection with optimized batch processing.\n",
    "    \"\"\"\n",
    "    DB_NAME = \"hrstud-bot\"\n",
    "    embed_fn = gemini_embedding_function\n",
    "    embed_fn.document_mode = True\n",
    "\n",
    "    db = chroma_client.get_or_create_collection(\n",
    "        name=DB_NAME,\n",
    "        metadata={\"model\": \"models/text-embedding-004\", \"dimension\": 768},\n",
    "        embedding_function=embed_fn\n",
    "    )\n",
    "\n",
    "    documents = [doc.page_content for doc in documents_list]\n",
    "    metadatas = [doc.metadata for doc in documents_list]\n",
    "    ids = [f\"{DB_NAME}_doc_{i}\" for i in range(len(documents))]\n",
    "\n",
    "    if db.count() == 0:\n",
    "        print(f\"Adding {len(documents)} documents to ChromaDB collection: {DB_NAME}\")\n",
    "\n",
    "        # Optimized batch size for Gemini API\n",
    "        BATCH_SIZE = 100\n",
    "        \n",
    "        for i in tqdm(range(0, len(documents), BATCH_SIZE), desc=\"Adding documents\", unit=\"batch\"):\n",
    "            batch_end = min(i + BATCH_SIZE, len(documents))\n",
    "            db.add(\n",
    "                documents=documents[i:batch_end],\n",
    "                metadatas=metadatas[i:batch_end],\n",
    "                ids=ids[i:batch_end]\n",
    "            )\n",
    "            # Rate limiting for API stability\n",
    "            time.sleep(0.2)\n",
    "\n",
    "        print(f\"\\nCollection '{DB_NAME}' now contains {db.count()} documents.\")\n",
    "    else:\n",
    "        print(f\"Collection '{DB_NAME}' already has {db.count()} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "891a657a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def persistent_client(embed_fn):\n",
    "    \"\"\"\n",
    "    Initialize persistent ChromaDB client.\n",
    "    \"\"\"\n",
    "    persist_dir = \"./output\"\n",
    "    chroma_client = chromadb.PersistentClient(path=persist_dir)\n",
    "\n",
    "    DB_NAME = \"hrstud-bot\"\n",
    "    collection = chroma_client.get_collection(DB_NAME, embedding_function=embed_fn)\n",
    "\n",
    "    print(f\"Connected to collection: {collection.name}\")\n",
    "    print(f\"Documents: {collection.count()}\")\n",
    "    print(f\"Metadata: {collection.metadata}\")\n",
    "    return embed_fn, collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05886a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Placeholder for helper function (used in the original snippet)\n",
    "def _no_answer_response():\n",
    "    \"\"\"Standard no-answer response.\"\"\"\n",
    "    return (\"Ispričavamo se, ali ne mogu pronaći relevantan odgovor u bazi znanja. \"\n",
    "            \"Molimo kontaktirajte odgovarajuću službu za dodatne informacije.\")\n",
    "\n",
    "# FastText model loading assumed to be successful from the previous block\n",
    "LID_MODEL = fasttext.load_model('./fasttext/lid.176.ftz') \n",
    "\n",
    "def get_article_hr(user_query, embed_fn, collection, client, user_language):\n",
    "    # Print language (kept for debugging consistency)\n",
    "    print(user_language.upper())\n",
    "    \n",
    "    # Switch to query mode when generating embeddings\n",
    "    embed_fn.document_mode = False\n",
    "\n",
    "    # Retrieve top 1 document (based on your n_results=1 in the original code)\n",
    "    # The result structure is a dict: {'ids': [[]], 'distances': [[]], 'documents': [[]], 'metadatas': [[]], ...}\n",
    "    n_results_to_fetch = 3 # Fetch more results for a richer context\n",
    "    result = collection.query(query_texts=[user_query], n_results=n_results_to_fetch)\n",
    "    \n",
    "    # Extract documents (list of passages) and metadatas (list of dicts)\n",
    "    all_passages = result[\"documents\"][0]\n",
    "    all_metadatas = result[\"metadatas\"][0]\n",
    "\n",
    "    query_oneline = user_query.replace(\"\\n\", \" \")\n",
    "    print(query_oneline)\n",
    "    \n",
    "    # 1. CONSTRUCT THE CONTEXT\n",
    "    context_list = []\n",
    "    # Use the metadata from the top result to define the main source link\n",
    "    # Assuming 'source_path' contains the URL or relevant file path\n",
    "    document_link = all_metadatas[0].get(\"source_path\", \"Link nije dostupan\")\n",
    "    \n",
    "    for i, (passage, metadata) in enumerate(zip(all_passages, all_metadatas)):\n",
    "        # Format the context for the model\n",
    "        source_name = metadata.get(\"source\", \"Nepoznat izvor\")\n",
    "        # I removed the redundant \"PASSAGE: \" wrapper that was causing issues\n",
    "        context_list.append(f\"--- Izvor: {source_name} (Dio {i+1} od {len(all_passages)}) ---\\n{passage.strip()}\")\n",
    "\n",
    "    # Join all context chunks into a single string\n",
    "    context = \"\\n\\n\".join(context_list)\n",
    "    \n",
    "    # 2. CONSTRUCT THE PROMPT\n",
    "    # The document_link is now a defined variable\n",
    "    prompt = f\"\"\"\n",
    "    Ti si ljubazan, precizan i informativan chatbot **Fakulteta Hrvatskih studija**. Tvoja je glavna zadaća odgovarati na pitanja studenata, potencijalnih studenata i osoblja o fakultetu, uključujući informacije o studijima, nastavi, smjerovima, prijavama, i općenitim informacijama o školi.\n",
    "\n",
    "    **KRITIČNA PRAVILA:**\n",
    "    1.  Koristi ISKLJUČIVO informacije iz dostavljene dokumentacije.\n",
    "    2.  Odgovaraj na **Hrvatskom jeziku**.\n",
    "    3.  Budi koncizan ali potpun — navedi sve relevantne detalje iz konteksta.\n",
    "    4.  Ako dokumentacija ne sadrži odgovor, jasno i ljubazno reci da ne možeš pronaći odgovor u bazi znanja i uputi na kontaktiranje odgovarajuće službe.\n",
    "    5.  **Ne smiješ koristiti fraze poput \"Naravno, mogu vam pomoći!\" ili \"Evo nekoliko informacija o...\". Odmah započni s relevantnim odgovorom.**\n",
    "\n",
    "    **FORMATIRANJE ODGOVORA:**\n",
    "    * Sve odgovore započni s **Izvorni link je [LINK](url)**, nakon čega slijedi prazan red.\n",
    "    * Nemoj navoditi izvorni link dokumenta samo URL. npr nemoj navoditi: Izvorni link: ./markdown/fhs.hr_predmet_opsv.md\n",
    "    * Koristi podebljani tekst za ključne pojmove (npr. **Upisi**, **Filozofija**, **Pročelnik**).\n",
    "    * Koristi popise (liste) za nabrajanje informacija (studiji, uvjeti, rokovi).\n",
    "    * Odgovori trebaju biti profesionalni i službeni, ali s ljubaznim tonom.\n",
    "\n",
    "    **DOSTUPNA DOKUMENTACIJA (Kontekst):**\n",
    "    {context}\n",
    "\n",
    "    **KORISNIČKO PITANJE:** {query_oneline}\n",
    "\n",
    "    **ODGOVOR:**\n",
    "    \"\"\"\n",
    "    \n",
    "    # 3. Call the model\n",
    "    answer = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        contents=prompt, # Use the full prompt\n",
    "        config={\n",
    "            \"max_output_tokens\": 2048,\n",
    "            \"temperature\": 0.2,\n",
    "            \"top_p\": 0.9\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Prepend the link as per your strict instruction, since Gemini might not format the first line perfectly\n",
    "    #final_response = f\"Izvorni link: {document_link}\\n\\n{answer.text.strip()}\"\n",
    "    \n",
    "    #return final_response\n",
    "\n",
    "    return answer.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e2725bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1095 markdown files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 100%|██████████| 1095/1095 [00:00<00:00, 12975.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully processed 1095 files into documents.\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/gemini-embedding-exp-03-07\n",
      "models/gemini-embedding-exp\n",
      "models/gemini-embedding-001\n",
      "Collection 'hrstud-bot' already has 1095 documents.\n"
     ]
    }
   ],
   "source": [
    "# USAGE EXAMPLE - Uncomment to run\n",
    "\n",
    "markdown_folder = \"./markdown\"\n",
    "# \n",
    "# # STEP 1: Parse and chunk documents (run once or when documents change)\n",
    "md_documents = parse_markdown_for_metadata(markdown_folder)\n",
    "# \n",
    "# # STEP 2: Create collection and add documents (run once)\n",
    "client = import_google_api()\n",
    "gemini_embedding_function = embedding_function(client)\n",
    "chroma_persistent_client = chromadb.PersistentClient(path=\"./output\")\n",
    "create_collection(chroma_persistent_client, gemini_embedding_function, md_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52583056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/gemini-embedding-exp-03-07\n",
      "models/gemini-embedding-exp\n",
      "models/gemini-embedding-001\n",
      "Connected to collection: hrstud-bot\n",
      "Documents: 1095\n",
      "Metadata: {'model': 'models/text-embedding-004', 'dimension': 768}\n",
      "HR\n",
      "Tko je Sandro Skansi?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Izvorni link je [https://www.fhs.hr/djelatnik/sandro.skansi](https://www.fhs.hr/djelatnik/sandro.skansi)\n",
       "\n",
       "**Sandro Skansi** je izvanredni profesor. Diplomirao je 2009. godine **Filozofiju** i **Kroatologiju** na Hrvatskim studijima, a doktorirao je 2013. godine na Filozofskom fakultetu u Zagrebu disertacijom iz logike. Glavni mu je jezik Python.\n",
       "\n",
       "Član je Hrvatskog filozofskog drustva, Hrvatskog logičkog udruženja i Association for the Advancement of Artificial Intelligence. Izradio je ili nadzirao izradu za produkcijske sustave umjetne inteligencije za Erste Banku, Iskon, Eurostat, A1 Telekom i HOK osiguranje.\n",
       "\n",
       "Na Fakultetu Hrvatskih studija nositelj je sljedećih kolegija:\n",
       "**Prijediplomski:**\n",
       "*   [Društveni mediji i neformalna logika](https://www.fhs.hr/predmet/dmnl)\n",
       "*   [Logika 2](https://www.fhs.hr/predmet/log2_a)\n",
       "*   [Socijalna filozofija](https://www.fhs.hr/predmet/socfil_b)\n",
       "*   [Uvod u umjetnu inteligenciju](https://www.fhs.hr/predmet/uuui)\n",
       "*   [Završni rad](https://www.fhs.hr/predmet/zavrad)\n",
       "\n",
       "**Diplomski:**\n",
       "*   [Filozofija politike](https://www.fhs.hr/predmet/filpol)\n",
       "*   [Suvremena filozofija](https://www.fhs.hr/predmet/suvfil_a)\n",
       "*   [Umjetna inteligencija i razumijevanje prirodnoga jezika](https://www.fhs.hr/predmet/uirpj)\n",
       "*   [Diplomski rad](https://www.fhs.hr/predmet/diprad_f)\n",
       "\n",
       "**Doktorski:**\n",
       "*   [Politička i pravna filozofija](https://www.fhs.hr/predmet/ppf_c)\n",
       "*   [Estetika](https://www.fhs.hr/predmet/est_a)\n",
       "\n",
       "Njegovi znanstveni interesi su:\n",
       "*   Zaključivanje\n",
       "*   Duboko učenje\n",
       "*   Povijest logike i kibernetike u istočnom bloku\n",
       "*   Filozofija uma"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# STEP 3: Query the system (run for each query)\n",
    "# \n",
    "client = import_google_api()\n",
    "gemini_embedding_function = embedding_function(client)\n",
    "embed_fn, collection = persistent_client(gemini_embedding_function)\n",
    "# \n",
    "user_query = \"Tko je Sandro Skansi?\"  # Example query\n",
    "response = get_article_hr(\n",
    "    user_query=user_query,\n",
    "    embed_fn=embed_fn,\n",
    "    collection=collection,\n",
    "    client=client,\n",
    "    user_language=\"HR\"\n",
    ")\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55d872d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################################################\n",
      "QUERY: Koje predmete predaje Mato Škerbić?\n",
      "############################################################\n",
      "HR\n",
      "Koje predmete predaje Mato Škerbić?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Izvorni link je [https://www.fhs.hr/predmet/filodg](https://www.fhs.hr/predmet/filodg)\n",
       "\n",
       "**Matija Mato Škerbić** je nositelj i izvođač (seminar) predmeta **Filozofija odgoja**.\n",
       "\n",
       "Izvorni link je [https://www.fhs.hr/predmet/nek](https://www.fhs.hr/predmet/nek)\n",
       "\n",
       "**Matija Mato Škerbić** je nositelj i izvođač (seminar) predmeta **Nova etička kultura**.\n",
       "\n",
       "Izvorni link je [https://www.fhs.hr/predmet/fis](https://www.fhs.hr/predmet/fis)\n",
       "\n",
       "**Matija Mato Škerbić** je nositelj i izvođač (seminar) predmeta **Filozofija igre i športa**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "############################################################\n",
      "QUERY: Koje predmete predaje Sandro Skansi?\n",
      "############################################################\n",
      "HR\n",
      "Koje predmete predaje Sandro Skansi?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Izvorni link je [https://www.fhs.hr/predmet/socfil_b](https://www.fhs.hr/predmet/socfil_b)\n",
       "\n",
       "**Sandro Skansi** je nositelj kolegija **Socijalna filozofija**. Predavanja iz kolegija Socijalna filozofija počinju u četvrtak 9.10.\n",
       "\n",
       "Izvorni link je [https://www.fhs.hr/predmet/suvfil_a](https://www.fhs.hr/predmet/suvfil_a)\n",
       "\n",
       "**Sandro Skansi** je nositelj i izvođač seminara iz kolegija **Suvremena filozofija**.\n",
       "\n",
       "Izvorni link je [https://www.fhs.hr/predmet/ppf_c](https://www.fhs.hr/predmet/ppf_c)\n",
       "\n",
       "**Sandro Skansi** je nositelj kolegija **Politička i pravna filozofija**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "############################################################\n",
      "QUERY: Tko predaje Opća povijest srednjeg vijeka?\n",
      "############################################################\n",
      "HR\n",
      "Tko predaje Opća povijest srednjeg vijeka?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Izvorni link je [https://www.fhs.hr/predmet/opsv](https://www.fhs.hr/predmet/opsv)\n",
       "\n",
       "**Opća povijest srednjega vijeka** se izvodi, a nositelj je izv. prof. dr. sc. **Marko Jerković**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ADVANCED: Test multiple queries\n",
    "# \n",
    "test_queries = [\n",
    "    \"Koje predmete predaje Mato Škerbić?\",\n",
    "    \"Koje predmete predaje Sandro Skansi?\",\n",
    "    \"Tko predaje Opća povijest srednjeg vijeka?\"\n",
    "]\n",
    "# \n",
    "for query in test_queries:\n",
    "    print(f\"\\n{'#'*60}\")\n",
    "    print(f\"QUERY: {query}\")\n",
    "    print(f\"{'#'*60}\")\n",
    "    response = get_article_hr(\n",
    "        user_query=query,\n",
    "        embed_fn=embed_fn,\n",
    "        collection=collection,\n",
    "        client=client,\n",
    "        user_language=\"HR\"\n",
    "    )\n",
    "    display(Markdown(response))\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
